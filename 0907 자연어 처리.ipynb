{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c33cd050",
   "metadata": {},
   "source": [
    "## 공용 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e6655ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이썬\n",
    "# ≥3.5 필수\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# 공통 모듈 임포트\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 깔끔한 그래프 출력을 위해 %matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "\n",
    "# 그림을 저장할 위치\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"classification\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "# 이미지를 저장할 디렉토리 생성\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "# 이미지 저장\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"그림 저장:\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "from matplotlib import font_manager, rc\n",
    "import platform\n",
    "\n",
    "path = \"c:/Windows/Fonts/malgun.ttf\"\n",
    "if platform.system() == 'Darwin':\n",
    "    rc('font', family='AppleGothic')\n",
    "elif platform.system() == 'Windows':\n",
    "    font_name = font_manager.FontProperties(fname=path).get_name()\n",
    "    rc('font', family=font_name)\n",
    "    \n",
    "    \n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "# Jupyter Notebook의 출력을 소수점 이하 3자리로 제한\n",
    "%precision 3\n",
    "\n",
    "# 그래픽 출력을 좀 더 고급화하기 위한 라이브러리\n",
    "import seaborn as sns\n",
    "\n",
    "# 과학 기술 통계 라이브러리\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "\n",
    "# 사이킷런 ≥0.20 필수\n",
    "# 0.20 이상 버전에서 데이터 변환을 위한 Transformer 클래스가 추가됨\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# 노트북 실행 결과를 동일하게 유지하기 위해 시드 고정\n",
    "# 데이터를 분할할 때 동일한 분할을 만들어 냄\n",
    "np.random.seed(21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42a69a3",
   "metadata": {},
   "source": [
    "## 텍스트 군집"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b94c66",
   "metadata": {},
   "source": [
    "### 디렉토리 내에서 .data 로 끝나는 파일 모두 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca5d9474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.\\\\data\\\\OpinosisDataset\\\\topics\\\\accuracy_garmin_nuvi_255W_gps.txt.data', '.\\\\data\\\\OpinosisDataset\\\\topics\\\\bathroom_bestwestern_hotel_sfo.txt.data', '.\\\\data\\\\OpinosisDataset\\\\topics\\\\battery-life_amazon_kindle.txt.data', '.\\\\data\\\\OpinosisDataset\\\\topics\\\\battery-life_ipod_nano_8gb.txt.data', '.\\\\data\\\\OpinosisDataset\\\\topics\\\\battery-life_netbook_1005ha.txt.data']\n"
     ]
    }
   ],
   "source": [
    "# data/OpinosisDataset/topics 디렉토리에서 데이터 파일 전부 읽기\n",
    "\n",
    "import glob, os\n",
    "import platform\n",
    "\n",
    "# 디렉토리의 이름(경로)을 생성\n",
    "path_name = \"\"\n",
    "if platform.system() == 'Darwin':\n",
    "    path_name = './data/OpinosisDataset/topics'\n",
    "elif platform.system() == 'Windows':\n",
    "    path_name = '.\\\\data\\\\OpinosisDataset\\\\topics'\n",
    "\n",
    "# 디렉토리 안의 모든 파일 이름을 list 로 생성\n",
    "all_file_name = glob.glob(os.path.join(path_name, \"*.data\"))\n",
    "print(all_file_name[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18ebe747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 51 entries, 0 to 50\n",
      "Data columns (total 2 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   filename      51 non-null     object\n",
      " 1   opinion_text  51 non-null     object\n",
      "dtypes: object(2)\n",
      "memory usage: 944.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "# 파일의 이름을 저장할 list \n",
    "file_name_list = []\n",
    "# 파일의 내용을 저장할 list\n",
    "opinion_text = []\n",
    "\n",
    "# 파일의 경로를 순회하면서 파일의 내용을 읽어서 하나로 만들기\n",
    "\n",
    "# 순회하면서 파일의 내용 읽기\n",
    "for file in all_file_name:\n",
    "    # 파일 이름에 해당하는 파일 읽기\n",
    "    df = pd.read_table(file, index_col = None, header = 0, encoding = 'latin1')\n",
    "    \n",
    "    # 파일의 이름만 추출 - 파일 주소에서 가장 뒤가 파일 이름\n",
    "    filename_ = file.split('\\\\')[-1]\n",
    "    # 확장자에 사용하는 . 을 기준으로 가장 앞이 파일의 이름\n",
    "    filename = filename_.split('.')[0]\n",
    "    \n",
    "    # 파일 이름과 내용을 리스트에 저장\n",
    "    file_name_list.append(filename)\n",
    "    opinion_text.append(df.to_string())\n",
    "    \n",
    "#print(file_name_list[:5])\n",
    "#print(opinion_text[1])\n",
    "\n",
    "# 파일 이름과 내용으로 DataFrame 을 생성\n",
    "document_df = pd.DataFrame({'filename' : file_name_list,'opinion_text' : opinion_text})\n",
    "document_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6706d4e3",
   "metadata": {},
   "source": [
    "### 피처 벡터화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f6fdfce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\USER\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1467)\t0.02328103871534355\n",
      "  (0, 385)\t0.02328103871534355\n",
      "  (0, 1472)\t0.05108217042656528\n",
      "  (0, 2656)\t0.02328103871534355\n",
      "  (0, 1359)\t0.021823791182120654\n",
      "  (0, 725)\t0.02328103871534355\n",
      "  (0, 4371)\t0.021823791182120654\n",
      "  (0, 4597)\t0.017985227788980915\n",
      "  (0, 4277)\t0.02063313325216223\n",
      "  (0, 1470)\t0.019626446316331432\n",
      "  (0, 1806)\t0.019626446316331432\n",
      "  (0, 4044)\t0.02328103871534355\n",
      "  (0, 1469)\t0.015099822028591765\n",
      "  (0, 2471)\t0.02328103871534355\n",
      "  (0, 2175)\t0.021823791182120654\n",
      "  (0, 4195)\t0.021823791182120654\n",
      "  (0, 4200)\t0.019626446316331432\n",
      "  (0, 234)\t0.02328103871534355\n",
      "  (0, 4045)\t0.02328103871534355\n",
      "  (0, 4096)\t0.04126626650432446\n",
      "  (0, 1535)\t0.04364758236424131\n",
      "  (0, 1998)\t0.02328103871534355\n",
      "  (0, 158)\t0.021823791182120654\n",
      "  (0, 1166)\t0.02328103871534355\n",
      "  (0, 3134)\t0.021823791182120654\n",
      "  :\t:\n",
      "  (50, 2030)\t0.016585088556344184\n",
      "  (50, 2039)\t0.00910671917345855\n",
      "  (50, 1657)\t0.009756197200387806\n",
      "  (50, 1323)\t0.009446650467709946\n",
      "  (50, 3298)\t0.02211345140845891\n",
      "  (50, 3434)\t0.017323294154534086\n",
      "  (50, 3915)\t0.012357254208071102\n",
      "  (50, 1825)\t0.004978572260899886\n",
      "  (50, 17)\t0.0051884315578633305\n",
      "  (50, 3104)\t0.006038743328650493\n",
      "  (50, 898)\t0.004878098600193903\n",
      "  (50, 4232)\t0.03885608524126891\n",
      "  (50, 1896)\t0.04911213539826537\n",
      "  (50, 4269)\t0.004502321988250776\n",
      "  (50, 2655)\t0.02638915864736737\n",
      "  (50, 3541)\t0.034043630273520316\n",
      "  (50, 556)\t0.011918814707439532\n",
      "  (50, 3956)\t0.007157427819144863\n",
      "  (50, 3569)\t0.005081958603751351\n",
      "  (50, 2741)\t0.007770345351925173\n",
      "  (50, 2423)\t0.007770345351925173\n",
      "  (50, 1398)\t0.004328308008979062\n",
      "  (50, 1355)\t0.09820359696525216\n",
      "  (50, 1805)\t0.06875698955757152\n",
      "  (50, 485)\t0.017592772431578245\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "# 텍스트에서 구두점 제거\n",
    "\n",
    "# 마침표, 느낌표 등에 해당하는 구두점의 ord\n",
    "remove_punc_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "#print(remove_punc_dict)\n",
    "# 인스턴스 생성\n",
    "lemmar = WordNetLemmatizer()\n",
    "\n",
    "# 토큰화하는 함수\n",
    "def LemTokens(tokens):\n",
    "    return [lemmar.lemmatize(token) for token in tokens]\n",
    "# 소문자로 바꿔서 토큰화하고 어근을 찾아오는 함수\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punc_dict)))\n",
    "\n",
    "# 피처 벡터화 적용\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(tokenizer = LemNormalize, stop_words = 'english',\n",
    "                            ngram_range = (1, 2), min_df = 0.05, max_df = 0.9)\n",
    "feature_vect = tfidf_vect.fit_transform(document_df['opinion_text'])\n",
    "\n",
    "# 피처 벡터화의 결과 확인\n",
    "print(feature_vect)\n",
    "# 문자에 어떤 단어가 어느 정도의 가중치를 가지고 있는지를\n",
    "# 희소 행렬로 생성\n",
    "# TF-IDF 방식이기 때문에 갯수가 아닌 가중치로 표현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56df59c1",
   "metadata": {},
   "source": [
    "### 군집 알고리즘 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aad0ef0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         filename  \\\n",
      "0   accuracy_garmin_nuvi_255W_gps   \n",
      "1  bathroom_bestwestern_hotel_sfo   \n",
      "2      battery-life_amazon_kindle   \n",
      "3      battery-life_ipod_nano_8gb   \n",
      "4     battery-life_netbook_1005ha   \n",
      "\n",
      "                                        opinion_text  cluster_labels  \n",
      "0                                                ...               7  \n",
      "1                                                ...               0  \n",
      "2                                                ...               6  \n",
      "3                                                ...               6  \n",
      "4                                                ...               6  \n"
     ]
    }
   ],
   "source": [
    "# 군집 알고리즘들 중 K-Means 적용\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# 지정한 갯수로 클러스터링을 수행 - 8개의 군집 생성\n",
    "km_cluster = KMeans(n_clusters = 8, max_iter = 1000, random_state = 21)\n",
    "km_cluster.fit(feature_vect)\n",
    "\n",
    "# 군집의 결과인 라벨(숫자) 가져오기\n",
    "cluster_label = km_cluster.labels_\n",
    "cluster_centers = km_cluster.cluster_centers_\n",
    "\n",
    "# 원본 데이터에 분류된 라벨을 더하기\n",
    "document_df['cluster_labels'] = cluster_label\n",
    "print(document_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c3f50de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>opinion_text</th>\n",
       "      <th>cluster_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>features_windows7</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>keyboard_netbook_1005ha</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>screen_garmin_nuvi_255W_gps</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>screen_ipod_nano_8gb</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>screen_netbook_1005ha</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>size_asus_netbook_1005ha</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>speed_windows7</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>video_ipod_nano_8gb</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       filename  \\\n",
       "11            features_windows7   \n",
       "19      keyboard_netbook_1005ha   \n",
       "34  screen_garmin_nuvi_255W_gps   \n",
       "35         screen_ipod_nano_8gb   \n",
       "36        screen_netbook_1005ha   \n",
       "41     size_asus_netbook_1005ha   \n",
       "44               speed_windows7   \n",
       "49          video_ipod_nano_8gb   \n",
       "\n",
       "                                         opinion_text  cluster_labels  \n",
       "11                                                ...               1  \n",
       "19                                                ...               1  \n",
       "34                                                ...               1  \n",
       "35                                                ...               1  \n",
       "36                                                ...               1  \n",
       "41                                                ...               1  \n",
       "44                                                ...               1  \n",
       "49                                                ...               1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 같은 군집에 속한 파일 이름 확인\n",
    "\n",
    "# 같은 군집의 파일 이름들을 가져온 다음 filename 순으로 정렬\n",
    "document_df[document_df['cluster_labels'] == 1].sort_values(by = 'filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "043e7ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         filename  \\\n",
      "0   accuracy_garmin_nuvi_255W_gps   \n",
      "1  bathroom_bestwestern_hotel_sfo   \n",
      "2      battery-life_amazon_kindle   \n",
      "3      battery-life_ipod_nano_8gb   \n",
      "4     battery-life_netbook_1005ha   \n",
      "\n",
      "                                        opinion_text  cluster_labels  \\\n",
      "0                                                ...               7   \n",
      "1                                                ...               0   \n",
      "2                                                ...               6   \n",
      "3                                                ...               6   \n",
      "4                                                ...               6   \n",
      "\n",
      "   cluster_labels_for_3  \n",
      "0                     1  \n",
      "1                     2  \n",
      "2                     1  \n",
      "3                     1  \n",
      "4                     1  \n"
     ]
    }
   ],
   "source": [
    "# 군집의 갯수 조절 - 8개가 아니라 3개\n",
    "\n",
    "km_cluster = KMeans(n_clusters = 3, max_iter = 1000, random_state = 21)\n",
    "km_cluster.fit(feature_vect)\n",
    "\n",
    "cluster_label = km_cluster.labels_\n",
    "cluster_centers = km_cluster.cluster_centers_\n",
    "\n",
    "document_df['cluster_labels_for_3'] = cluster_label\n",
    "\n",
    "# 클러스터별로 정렬\n",
    "document_df.sort_values(by = 'cluster_labels_for_3')\n",
    "\n",
    "print(document_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0119729c",
   "metadata": {},
   "source": [
    "### 군집을 생성한(분류한) 핵심 단어 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f0c94dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클러스터의 핵심 단어를 찾는 함수\n",
    "# 모델 종류, 클러스터의 수, 몇 개의 단어를 찾을지 등을 입력\n",
    "def get_cluster_center(cluster_model, cluster_data, \n",
    "                       feature_names, cluster_num, top_n_features = 10):\n",
    "    cluster_details = {}\n",
    "    \n",
    "    # 군집 중심과의 거리가 먼 단어 순서대로 정렬해서 저장\n",
    "    # cluster_model.cluster_centers_ 가 중심점의 좌료\n",
    "    centroid_feature_order_far = cluster_model.cluster_centers_.argsort()[:, ::-1]\n",
    "    \n",
    "    # \n",
    "    for clust_num in range(cluster_num):\n",
    "        cluster_details[clust_num] = {}\n",
    "        cluster_details[clust_num]['cluster'] = clust_num\n",
    "        \n",
    "        # 지정한 갯수만큼 중요 피처 추출\n",
    "        top_feature_indexs = centroid_feature_order_far[clust_num, :top_n_features]\n",
    "        # 저장되어 있던 단어를 반영\n",
    "        top_features = [feature_names[idx] for idx in top_feature_indexs]\n",
    "        \n",
    "        # 중심점과의 거리 저장\n",
    "        top_feature_values = cluster_model.cluster_centers_[clust_num,\n",
    "                                            top_feature_indexs].tolist()\n",
    "        cluster_details[clust_num]['top_features'] = top_features\n",
    "        cluster_details[clust_num]['top_feature_values'] = top_feature_values\n",
    "        filenames = cluster_data[cluster_data['cluster_labels'] == clust_num]['filename']\n",
    "        filenames = filenames.values.tolist()\n",
    "        cluster_details[clust_num]['filenames'] = filenames\n",
    "        \n",
    "    return cluster_details\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af1fa170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클러스터 별 핵심 단어를 출력하는 함수\n",
    "def print_cluster_center(cluster_details):\n",
    "    for clust_num, cluster_detail in cluster_details.items():\n",
    "        print('### ', clust_num, ' ###')\n",
    "        print('핵심 단어 :', cluster_detail['top_features'])\n",
    "        print('파일명 :', cluster_detail['filenames'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6b6626a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###  0  ###\n",
      "핵심 단어 : ['interior', 'seat', 'mileage', 'comfortable', 'gas', 'gas mileage', 'transmission', 'car', 'performance', 'quality']\n",
      "파일명 : ['bathroom_bestwestern_hotel_sfo', 'rooms_bestwestern_hotel_sfo', 'rooms_swissotel_chicago', 'room_holiday_inn_london']\n",
      "###  1  ###\n",
      "핵심 단어 : ['screen', 'battery', 'keyboard', 'battery life', 'life', 'kindle', 'direction', 'video', 'size', 'voice']\n",
      "파일명 : ['features_windows7', 'keyboard_netbook_1005ha', 'screen_garmin_nuvi_255W_gps', 'screen_ipod_nano_8gb', 'screen_netbook_1005ha', 'size_asus_netbook_1005ha', 'speed_windows7', 'video_ipod_nano_8gb']\n",
      "###  2  ###\n",
      "핵심 단어 : ['room', 'hotel', 'service', 'staff', 'food', 'location', 'bathroom', 'clean', 'price', 'parking']\n",
      "파일명 : ['gas_mileage_toyota_camry_2007', 'mileage_honda_accord_2008', 'transmission_toyota_camry_2007']\n"
     ]
    }
   ],
   "source": [
    "# 피처 이름을 전부 가져오기\n",
    "\n",
    "feature_names = tfidf_vect.get_feature_names_out()\n",
    "#print(feature_names)\n",
    "\n",
    "# 클러스터에 대한 자세한 정보 가져오기\n",
    "cluster_details = get_cluster_center(cluster_model = km_cluster,\n",
    "                                     cluster_data = document_df,\n",
    "                                     feature_names = feature_names,\n",
    "                                     cluster_num = 3,\n",
    "                                     top_n_features = 10)\n",
    "\n",
    "# 클러스터에 대한 정보 출력\n",
    "print_cluster_center(cluster_details)\n",
    "# 결과 - 해당 파일들에는 포함되는 단어이지만 다른 파일에는 없는 단어들"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693fabd4",
   "metadata": {},
   "source": [
    "## 문장의 유사도 측정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8928163c",
   "metadata": {},
   "source": [
    "### 코사인 유사도를 구하는 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b209070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코사인 유사도를 구하는 함수\n",
    "def cos_similarity(vector1, vector2):\n",
    "    dot_product = np.dot(vector1, vector2)\n",
    "    l2_norm = (np.sqrt(sum(np.square(vector1))) * np.sqrt(sum(np.square(vector2))))\n",
    "    similarity = dot_product / l2_norm\n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42a93a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3)\t0.6088450986844796\n",
      "  (0, 1)\t0.6088450986844796\n",
      "  (0, 7)\t0.35959372325985667\n",
      "  (0, 4)\t0.35959372325985667\n",
      "####\n",
      "  (0, 6)\t0.55249004708441\n",
      "  (0, 0)\t0.55249004708441\n",
      "  (0, 5)\t0.42018292148905534\n",
      "  (0, 7)\t0.3263095219528963\n",
      "  (0, 4)\t0.3263095219528963\n",
      "[[0.    0.609 0.    0.609 0.36  0.    0.    0.36 ]]\n",
      "[[0.552 0.    0.    0.    0.326 0.42  0.552 0.326]]\n"
     ]
    }
   ],
   "source": [
    "# 샘플 데이터 생성\n",
    "document_list = ['i need to go home', 'i need to have some',\n",
    "                'i need to get some things']\n",
    "\n",
    "# 전처리\n",
    "\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "feature_vect = tfidf_vect.fit_transform(document_list)\n",
    "print(feature_vect[0])\n",
    "print('####')\n",
    "print(feature_vect[2])\n",
    "# 결과가 희소 행렬 형태이므로 아직은 거리를 계산할 수 없음\n",
    "# 거리를 계산하려면 서로 구조가 같아야 하지만 문장 별로 피처의 수가 다름\n",
    "\n",
    "# 희소 행렬을 밀집 행렬로 변환 - 서로 구조가 같아짐\n",
    "feature_vect_dense = feature_vect.todense()\n",
    "print(feature_vect_dense[0])\n",
    "print(feature_vect_dense[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "916e670b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 1과 2의 유사도 : 0.28155035771770787\n",
      "문장 1과 3의 유사도 : 0.23467771186837183\n",
      "문장 2과 3의 유사도 : 0.4673070015402796\n"
     ]
    }
   ],
   "source": [
    "# 문장 사이의 거리 계산\n",
    "\n",
    "# 거리 계산을 위해 1차원으로 변환\n",
    "vect1 = np.array(feature_vect_dense[0]).reshape(-1, )\n",
    "vect2 = np.array(feature_vect_dense[1]).reshape(-1, )\n",
    "vect3 = np.array(feature_vect_dense[2]).reshape(-1, )\n",
    "\n",
    "print('문장 1과 2의 유사도 :', cos_similarity(vect1, vect2))\n",
    "print('문장 1과 3의 유사도 :', cos_similarity(vect1, vect3))\n",
    "print('문장 2과 3의 유사도 :', cos_similarity(vect2, vect3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71d32cf",
   "metadata": {},
   "source": [
    "### sklearn 을 활용해서 코사인 유사도 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c239d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.    0.282 0.235]]\n"
     ]
    }
   ],
   "source": [
    "# sklearn 의 코사인 유사도 계산 API 를 사용해서 유사도 계산\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 밀집 행렬로 변환하지 않고 희소 행렬 상태에서도 계산 가능\n",
    "similarity_simple_pair = cosine_similarity(feature_vect[0],\n",
    "                                          feature_vect)\n",
    "print(similarity_simple_pair)\n",
    "# 결과 - [[1.    0.282 0.235]]\n",
    "# 자기 자신과는 결과가 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce29ba05",
   "metadata": {},
   "source": [
    "### 문서 군집의 코사인 유사도 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b85db8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.    0.    0.005 ... 0.    0.002 0.001]\n",
      " [0.009 0.    0.    ... 0.01  0.    0.   ]\n",
      " [0.    0.003 0.    ... 0.    0.    0.   ]\n",
      " ...\n",
      " [0.006 0.    0.    ... 0.    0.    0.   ]\n",
      " [0.015 0.    0.    ... 0.    0.    0.   ]\n",
      " [0.011 0.    0.    ... 0.014 0.    0.   ]]\n"
     ]
    }
   ],
   "source": [
    "tfidf_vect = TfidfVectorizer(tokenizer = LemNormalize, stop_words = 'english',\n",
    "                            ngram_range = (1, 2), min_df = 0.05, max_df = 0.9)\n",
    "feature_vect = tfidf_vect.fit_transform(document_df['opinion_text'])\n",
    "\n",
    "km_cluster = KMeans(n_clusters = 8, max_iter = 1000, random_state = 21)\n",
    "km_cluster.fit(feature_vect)\n",
    "\n",
    "cluster_label = km_cluster.labels_\n",
    "# 각 군집의 centroid\n",
    "cluster_centers = km_cluster.cluster_centers_\n",
    "\n",
    "# 이전의 문서 데이터 다시 사용\n",
    "document_df['cluster_labels'] = cluster_label\n",
    "print(cluster_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4fbf6b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.    0.037 0.071 0.121 0.073 0.071 0.304 0.064]]\n",
      "[[0.037 0.01  0.04  0.036 0.035 0.028 0.048 0.038 0.062 0.068]]\n"
     ]
    }
   ],
   "source": [
    "# 1번 클러스터의 문서들 간 코사인 유사도 확인\n",
    "\n",
    "# 1번 클러스터의 인덱스 가져오기\n",
    "hotel_indexs = document_df[document_df['cluster_labels'] == 1].index\n",
    "#print('1번 클러스터의 인덱스 :', hotel_indexs)\n",
    "\n",
    "# 유사도 계산 - 같은 그룹 내에서 유사도 계산\n",
    "similarity_pair = cosine_similarity(feature_vect[hotel_indexs[0]],\n",
    "                                    feature_vect[hotel_indexs])\n",
    "print(similarity_pair)\n",
    "# 그룹과 관계 없이 유사도 계산\n",
    "similarity_pair = cosine_similarity(feature_vect[hotel_indexs[0]],\n",
    "                                    feature_vect[:10])\n",
    "print(similarity_pair)\n",
    "\n",
    "# 결과 - 동일 군집 내에서는 유사도가 높게 나타나지만 다른 군집에 대해서는\n",
    "# 낮은 유사도를 보임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342aaf6f",
   "metadata": {},
   "source": [
    "### 한글 문장의 유사도 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "79dcf1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['오늘', '은', '날씨', '가', '매우', '좋다'], ['오늘', '도', '날씨', '가', '덥다'], ['오늘', '은', '목요일', '이다'], ['서울', '날씨', '는', '맑음'], ['오늘', '점심', '뭐', '먹지']]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from konlpy.tag import Okt\n",
    "# TfidfVectorizer 로 바꾸거나 Okt 대신 Twitter 를 사용해도 무방함\n",
    "\n",
    "okt = Okt()\n",
    "vectorizer = CountVectorizer(min_df = 0.05)\n",
    "\n",
    "# 문장 생성\n",
    "contents = ['오늘은 날씨가 매우 좋다', '오늘도 날씨가 덥다',\n",
    "           '오늘은 목요일이다', '서울 날씨는 맑음',\n",
    "           '오늘 점심 뭐 먹지']\n",
    "\n",
    "# 한글 문장 토큰화\n",
    "contents_tokens = [okt.morphs(row) for row in contents]\n",
    "print(contents_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "11a58396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' 오늘 은 날씨 가 매우 좋다', ' 오늘 도 날씨 가 덥다', ' 오늘 은 목요일 이다', ' 서울 날씨 는 맑음', ' 오늘 점심 뭐 먹지']\n"
     ]
    }
   ],
   "source": [
    "# 토큰화된 결과를 가지고 다시 문장을 생성\n",
    "# 피처 벡터화를 적용하기 위해서\n",
    "\n",
    "contents_for_vect = []\n",
    "\n",
    "# 토큰 단위로 구분된 문장을 생성\n",
    "for content in contents_tokens:\n",
    "    sentence = ''\n",
    "    for word in content:\n",
    "        sentence += ' ' + word\n",
    "    contents_for_vect.append(sentence)\n",
    "    \n",
    "print(contents_for_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8e64bffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['날씨' '덥다' '맑음' '매우' '먹지' '목요일' '서울' '오늘' '이다' '점심' '좋다']\n",
      "[[1 1 0 1 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 0 1 0]\n",
      " [1 0 0 0 0]\n",
      " [0 0 0 0 1]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 1 0]\n",
      " [1 1 1 0 1]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 0 1]\n",
      " [1 0 0 0 0]]\n",
      "[[1 0 0 1 0 0 0 1 0 0 1]\n",
      " [1 1 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 1 1 0 0]\n",
      " [1 0 1 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 1 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# 피처 벡터화\n",
    "\n",
    "X = vectorizer.fit_transform(contents_for_vect)\n",
    "# 피처 확인\n",
    "print(vectorizer.get_feature_names_out())\n",
    "\n",
    "# 피처 벡터화가 된 후의 문장 결과 확인\n",
    "print(X.toarray().transpose())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0d8cb71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' 오늘 점심 이후 오후 서울 의 날씨 는 매우 덥다']\n"
     ]
    }
   ],
   "source": [
    "# 유사도를 측정할 문장 생성\n",
    "new_content = ['오늘 점심 이후 오후 서울의 날씨는 매우 덥다']\n",
    "new_content_tokens = [okt.morphs(row) for row in new_content]\n",
    "\n",
    "# 토큰화된 문장 조립\n",
    "new_content_for_vect = []\n",
    "for content in new_content_tokens:\n",
    "    sentence = ''\n",
    "    for word in content:\n",
    "        sentence += ' ' + word\n",
    "    new_content_for_vect.append(sentence)\n",
    "\n",
    "print(new_content_for_vect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5afeadd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 1 0 0 1 1 0 1 0]]\n",
      "['날씨' '덥다' '맑음' '매우' '먹지' '목요일' '서울' '오늘' '이다' '점심' '좋다']\n",
      "  (0, 0)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 9)\t1\n"
     ]
    }
   ],
   "source": [
    "# 테스트 문장을 피처 벡터화\n",
    "\n",
    "new_content_vect = vectorizer.transform(new_content_for_vect)\n",
    "print(new_content_vect.toarray())\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(new_content_vect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "84124347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 데이터와 훈련 데이터 거리 확인\n",
    "\n",
    "import scipy as sp\n",
    "\n",
    "#거리 구해주는 함수\n",
    "def dist_raw(v1, v2):\n",
    "    delta = v1 - v2\n",
    "    return sp.linalg.norm(delta.toarray())\n",
    "\n",
    "best_doc = None\n",
    "best_dist = 65535\n",
    "best_i = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "65337fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== 0 번째 문장과의 거리: 2.00 : 오늘은 날씨가 매우 좋다\n",
      "== 1 번째 문장과의 거리: 1.73 : 오늘도 날씨가 덥다\n",
      "== 2 번째 문장과의 거리: 2.65 : 오늘은 목요일이다\n",
      "== 3 번째 문장과의 거리: 2.24 : 서울 날씨는 맑음\n",
      "== 4 번째 문장과의 거리: 2.24 : 오늘 점심 뭐 먹지\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(contents)):\n",
    "    post_vec = X.getrow(i)\n",
    "    d = dist_raw(post_vec, new_content_vect)\n",
    "    print(\"== %i 번째 문장과의 거리: %.2f : %s\" %(i,d,contents[i]))\n",
    "    if d<best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f9154d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
